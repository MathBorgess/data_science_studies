{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MathBorgess/data_science_studies/blob/main/deep_learning/recurrent/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftLK5YepIWv7",
        "outputId": "9b256ac3-f42f-4da6-ccd2-ce1d35f11549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2638744/2638744 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Si quieres sonar como un hablante nativo, debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un músico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado.',\n",
              " 'If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#attention rnn\n",
        "\n",
        "#https://www.tensorflow.org/text/tutorials/nmt_with_attention?hl=pt-br#shape_checker\n",
        "\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file( 'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', extract=True)\n",
        "path_to_file = pathlib.Path(path_to_zip).parent/'spa-eng/spa.txt'\n",
        "\n",
        "def load_data(path):\n",
        "  text = path.read_text(encoding='utf-8')\n",
        "\n",
        "  lines = text.splitlines()\n",
        "  pairs = [line.split('\\t') for line in lines]\n",
        "\n",
        "  inp = [inp for targ, inp in pairs]\n",
        "  targ = [targ for targ, inp in pairs]\n",
        "\n",
        "  return targ, inp\n",
        "\n",
        "en_dataset, spa_dataset = load_data(path_to_file)\n",
        "[spa_dataset[-1], en_dataset[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fgJzZjz6IWwB"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((en_dataset, spa_dataset)).shuffle(len(en_dataset)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFgqLULyIWwC",
        "outputId": "786b0ed6-42ed-40f4-a4f2-1c52bad7debf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'[START] hell ? [END]'>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tf.strings.unicode_transcode(tf.constant('Hello?'), input_encoding='UTF-8', output_encoding='UTF-8')\n",
        "\n",
        "def tf_lower_and_split_punct(text):\n",
        "  text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "tf_lower_and_split_punct(tf.constant('Hellá?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1XubHPyIWwC",
        "outputId": "e268cceb-6401-483c-d1ea-064bfc9c2fac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['', '[UNK]', '[START]', '[END]', '.', 'de', 'que', 'a'],\n",
              " ['', '[UNK]', '[START]', '[END]', '.', 'the', 'i', 'to']]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "input_text_processor.adapt(en_dataset)\n",
        "\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_lower_and_split_punct,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(spa_dataset)\n",
        "\n",
        "[output_text_processor.get_vocabulary()[:8],input_text_processor.get_vocabulary()[:8]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GNfgHovFIWwD",
        "outputId": "20f62c8e-ba1d-4d8d-d4f2-5669675ea4f6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[START] i love you [END]'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokenize = input_text_processor(tf.constant('I love you'))\n",
        "tokenize_decoder = np.array(input_text_processor.get_vocabulary())\n",
        "' '.join(tokenize_decoder[tokenize.numpy()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vhPeKRovIWwE"
      },
      "outputs": [],
      "source": [
        "#Some constants\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gPfAH6T1IWwE"
      },
      "outputs": [],
      "source": [
        "#the models\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size, embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state\n",
        "\n",
        "# https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "\n",
        "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "        self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "    def call(self, query, value, mask):\n",
        "        w1_query = self.W1(query)\n",
        "        w2_key = self.W2(value)\n",
        "\n",
        "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "\n",
        "        context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, mask],\n",
        "        return_attention_scores = True\n",
        "        )\n",
        "\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "import typing\n",
        "#classes to use in Decoder:\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: typing.Any\n",
        "  enc_output: typing.Any\n",
        "  mask: typing.Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: typing.Any\n",
        "  attention_weights: typing.Any\n",
        "\n",
        "# O decodificador recebe a saída completa do codificador.\n",
        "# Ele usa um RNN para rastrear o que gerou até agora.\n",
        "# Ele usa sua saída RNN como a consulta para chamar a atenção sobre a saída do codificador, produzindo o vetor de contexto.\n",
        "# Ele combina a saída RNN e o vetor de contexto para gerar o \"vetor de atenção\".\n",
        "# Ele gera previsões logit para o próximo token com base no \"vetor de atenção\".\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, output_vocabulary_size, embedding_dim, decode_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.decode_units = decode_units\n",
        "        self.output_vocabulary_size = output_vocabulary_size\n",
        "\n",
        "        # For Step 1. The embedding layer convets token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.output_vocabulary_size, embedding_dim)\n",
        "\n",
        "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "        self.gru = tf.keras.layers.GRU(self.decode_units,\n",
        "                                    return_sequences=True,\n",
        "                                    return_state=True,\n",
        "                                    recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        # For step 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = BahdanauAttention(self.decode_units)\n",
        "\n",
        "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "        self.Wc = tf.keras.layers.Dense(self.decode_units, activation=tf.math.tanh,\n",
        "                                        use_bias=False)\n",
        "\n",
        "        # For step 5. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.fc = tf.keras.layers.Dense(self.output_vocabulary_size)\n",
        "\n",
        "\n",
        "    def call(self, inputs: DecoderInput, state=None)  -> typing.Tuple[DecoderOutput, tf.Tensor]:\n",
        "        vectors = self.embedding(inputs.new_tokens)\n",
        "\n",
        "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "        # Step 3. Use the RNN output as the query for the attention over the encoder output.\n",
        "        context_vector, attention_weights = self.attention(query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "\n",
        "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "        attention_weights = self.Wc(context_and_rnn_output)\n",
        "        logits = self.fc(attention_weights)\n",
        "\n",
        "        return DecoderOutput(logits=logits, attention_weights= attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85tloaIfMok5",
        "outputId": "e7b92bf3-e6a7-493e-a4d5-1f7ff4a514df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 1, 5000), dtype=float32, numpy=\n",
              "array([[[ 0.00108569,  0.00162426,  0.00185247, ..., -0.00279206,\n",
              "          0.00406274,  0.00171106]]], dtype=float32)>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example_tokens = input_text_processor(tf.constant('I love you'))\n",
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor([tf.constant('I love you')])\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "\n",
        "example_output_tokens = output_text_processor([tf.constant('Te amo')])\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n",
        "\n",
        "[first_token, start_index]\n",
        "\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "dec_result.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mg66wxkeIWwF"
      },
      "outputs": [],
      "source": [
        "#Defining the training loop\n",
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)\n",
        "\n",
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor,\n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    return self._train_step(inputs)\n",
        "\n",
        "  def _preprocess(self, input_text, target_text):\n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "\n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "\n",
        "    target_mask = target_tokens != 0\n",
        "\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "  def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs\n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      # Encode the input\n",
        "      enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "      # Initialize the decoder's state to the encoder's final state.\n",
        "      # This only works if the encoder and decoder have the same number of\n",
        "      # units.\n",
        "      dec_state = enc_state\n",
        "      loss = tf.constant(0.0)\n",
        "\n",
        "      for t in tf.range(max_target_length-1):\n",
        "        # Pass in two tokens from the target sequence:\n",
        "        # 1. The current input to the decoder.\n",
        "        # 2. The target for the decoder's next prediction.\n",
        "        new_tokens = target_tokens[:, t:t+2]\n",
        "        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                              enc_output, dec_state)\n",
        "        loss = loss + step_loss\n",
        "\n",
        "      # Average the loss over all non padding tokens.\n",
        "      average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables\n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    # Run the decoder one step.\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                enc_output=enc_output,\n",
        "                                mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "    # `self.loss` returns the total for non-padded tokens\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "\n",
        "    return step_loss, dec_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rKPWQ-QIIWwG"
      },
      "outputs": [],
      "source": [
        "translator = TrainTranslator(embedding_dim=embedding_dim,\n",
        "                            input_text_processor=input_text_processor,\n",
        "                            output_text_processor=output_text_processor,\n",
        "                            units=units,\n",
        "                            use_tf_function=True)\n",
        "translator.compile(loss=MaskedLoss(), optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ll0t4ObIWwH",
        "outputId": "b10beaaf-50f8-4c3d-916b-81db53ab5aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/4\n",
            "1859/1859 [==============================] - 597s 320ms/step - batch_loss: 0.6893\n",
            "Epoch 2/4\n",
            "1859/1859 [==============================] - 573s 308ms/step - batch_loss: 0.5761\n",
            "Epoch 3/4\n",
            "1859/1859 [==============================] - 577s 310ms/step - batch_loss: 0.4826\n",
            "Epoch 4/4\n",
            "1859/1859 [==============================] - 583s 313ms/step - batch_loss: 0.4081\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7b3e16a45ba0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator.fit(dataset, epochs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5L556kWdo05a"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))\n",
        "\n",
        "  def sample(self, logits, temperature):\n",
        "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "    if temperature == 0.0:\n",
        "      new_tokens = tf.argmax(logits, axis=-1)\n",
        "    else:\n",
        "      logits = tf.squeeze(logits, axis=1)\n",
        "      new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                          num_samples=1)\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "  def tokens_to_text(self, result_tokens):\n",
        "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "\n",
        "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                        axis=1, separator=' ')\n",
        "    result_text = tf.strings.strip(result_text)\n",
        "    return result_text\n",
        "\n",
        "  def translate(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "    batch_size = tf.shape(input_text)[0]\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    dec_state = enc_state\n",
        "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "    result_tokens = []\n",
        "    attention = []\n",
        "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                              enc_output=enc_output,\n",
        "                              mask=(input_tokens!=0))\n",
        "\n",
        "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "      attention.append(dec_result.attention_weights)\n",
        "\n",
        "      new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "      # If a sequence produces an `end_token`, set it `done`\n",
        "      done = done | (new_tokens == self.end_token)\n",
        "      # Once a sequence is done it only produces 0-padding.\n",
        "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "      # Collect the generated tokens\n",
        "      result_tokens.append(new_tokens)\n",
        "\n",
        "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "        break\n",
        "\n",
        "    # Convert the list of generates token ids to a list of strings.\n",
        "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "    result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "    if return_attention:\n",
        "      attention_stack = tf.concat(attention, axis=1)\n",
        "      return {'text': result_text, 'attention': attention_stack}\n",
        "    else:\n",
        "      return {'text': result_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FACFCwgDqiMy",
        "outputId": "43c8965e-d0b8-44aa-e6bd-db24bfd044ce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'de verdad que quiero saber qu es por mi trabajo con el suyo de trabajo .'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = Translator(encoder=translator.encoder,\n",
        "                   decoder=translator.decoder,\n",
        "                   input_text_processor=translator.input_text_processor,\n",
        "                   output_text_processor=translator.output_text_processor)\n",
        "\n",
        "model.translate(input_text=tf.constant(['I really want to know what is happen with my job']))['text'][0].numpy().decode()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
