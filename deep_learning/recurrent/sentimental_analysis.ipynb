{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "imdb = tf.keras.datasets.imdb\n",
    "\n",
    "train_data, test_data = imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word_index = {k: (v+3) for k,v in imdb.get_word_index().items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for key, value in word_index.items()])\n",
    "\n",
    "train_dataset_texts = []\n",
    "train_dataset_labels = []\n",
    "for index in range(len(train_data[0])):\n",
    "    train_dataset_texts.append(' '.join([ reverse_word_index.get(i, '?') for i in train_data[0][index]]))\n",
    "    train_dataset_labels.append(train_data[1][index])\n",
    "\n",
    "test_dataset_texts = []\n",
    "test_dataset_labels = []\n",
    "for index in range(len(test_data[0])):\n",
    "    test_dataset_texts.append(' '.join([ reverse_word_index.get(i, '?') for i in test_data[0][index]]))\n",
    "    test_dataset_labels.append(test_data[1][index])\n",
    "\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_dataset_texts, train_dataset_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_dataset_texts, test_dataset_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "\n",
    "encoder = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size)\n",
    "encoder.adapt(train_dataset.map(lambda text, label: text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentalClassifier(tf.keras.Model):\n",
    "    def __init__(self, encoder, dense_units):\n",
    "        super(SentimentalClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.embedding = tf.keras.layers.Embedding(input_dim=len(encoder.get_vocabulary()),\n",
    "                                                    output_dim=dense_units,\n",
    "                                                    mask_zero=True)\n",
    "        # It performs average pooling across the temporal dimension of the input data,\n",
    "        # reducing the spatial dimensionality of the data while preserving important features.\n",
    "        # self.averager = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.gru = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(dense_units, return_state=True, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.Dense(dense_units, activation='relu')\n",
    "        self.outputer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs, state=None):\n",
    "        vector = self.encoder(inputs)\n",
    "        vector = self.embedding(vector)\n",
    "        y, forward_state, backward_state = self.gru(vector, initial_state=state)\n",
    "        y = self.dense(y)\n",
    "        output = self.outputer(y)\n",
    "        #the second tf.concat param is the axis of concat, 0 will only append, 1 will append line by line and so\n",
    "        return output, tf.concat([forward_state, backward_state], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 19, 1), dtype=float32, numpy=\n",
       " array([[[0.4965589 ],\n",
       "         [0.50012517],\n",
       "         [0.5004365 ],\n",
       "         [0.4931547 ],\n",
       "         [0.4947679 ],\n",
       "         [0.4989229 ],\n",
       "         [0.5025305 ],\n",
       "         [0.49466473],\n",
       "         [0.49846768],\n",
       "         [0.50109106],\n",
       "         [0.50188833],\n",
       "         [0.4969383 ],\n",
       "         [0.49871266],\n",
       "         [0.50345856],\n",
       "         [0.49946272],\n",
       "         [0.4973263 ],\n",
       "         [0.49614328],\n",
       "         [0.49713218],\n",
       "         [0.49834543]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128), dtype=float32, numpy=\n",
       " array([[ 1.41319772e-03, -8.33016541e-03,  5.35984291e-03,\n",
       "         -6.95339264e-03, -3.09642055e-03, -1.03756376e-02,\n",
       "         -4.15473152e-03, -2.42858753e-03,  1.05134500e-02,\n",
       "          4.63201711e-03,  1.34592196e-02, -2.94765062e-03,\n",
       "         -4.53861617e-03, -2.78137065e-02, -1.90704539e-02,\n",
       "         -2.03352980e-03, -8.59928690e-03,  1.47838760e-02,\n",
       "          8.87841266e-03,  4.18538228e-03, -1.04959197e-02,\n",
       "          4.10443079e-03,  1.68783870e-02, -1.33995293e-02,\n",
       "         -8.82677361e-03, -5.04867465e-04,  1.02152843e-02,\n",
       "         -3.26298759e-05,  1.15993507e-02,  1.88727267e-02,\n",
       "         -1.36818048e-02, -2.11423598e-02, -3.88735160e-03,\n",
       "          2.23177243e-02,  8.70735757e-03, -7.61748012e-03,\n",
       "          3.09630716e-03, -3.10608838e-03,  6.68671913e-03,\n",
       "          1.97037216e-02, -1.36930114e-02, -6.68125576e-04,\n",
       "          1.07661868e-02, -3.39745963e-03,  1.30108204e-02,\n",
       "          1.65961962e-02, -3.97548499e-03,  8.48021638e-03,\n",
       "         -4.79685841e-03,  1.78537183e-02, -6.31300174e-03,\n",
       "          7.61553738e-03,  4.30303253e-03, -5.98239712e-05,\n",
       "          4.11133096e-03, -3.64763942e-03, -4.42525977e-03,\n",
       "         -1.50974058e-02,  1.42990015e-02, -7.24725472e-03,\n",
       "          5.67535032e-03,  2.58307569e-02, -1.48129538e-02,\n",
       "          1.16340891e-02, -1.00097936e-02,  1.41501520e-02,\n",
       "         -1.12933321e-02,  8.19190592e-03,  4.17196611e-03,\n",
       "         -1.21134110e-02,  4.03057132e-03,  6.65030116e-03,\n",
       "          1.53326802e-02, -2.53122486e-03,  6.32295012e-03,\n",
       "          2.67980341e-03, -7.93659594e-04,  1.03755435e-02,\n",
       "          1.40016135e-02, -9.99958999e-03,  1.02488585e-02,\n",
       "          1.64715964e-02,  6.38539530e-03,  7.22154183e-03,\n",
       "          6.62305532e-03,  1.13173947e-02, -2.97005288e-04,\n",
       "         -1.63136963e-02, -9.92453843e-03,  3.80361779e-03,\n",
       "          1.93089992e-02,  3.10251210e-03,  1.71142742e-02,\n",
       "          1.62671804e-02, -5.15893195e-03,  1.62601527e-02,\n",
       "         -1.96186770e-02,  1.12622781e-02,  1.59363225e-02,\n",
       "         -8.79308209e-03,  1.39226075e-02,  1.22608282e-02,\n",
       "          1.87714025e-02, -2.17065681e-03,  3.35896201e-03,\n",
       "         -3.60297912e-04, -1.49108600e-02,  2.19014939e-04,\n",
       "          1.69836208e-02, -1.60501339e-03,  1.43646300e-02,\n",
       "          9.07025393e-03,  4.29821573e-03,  1.26959756e-02,\n",
       "         -6.19049836e-03,  8.97485949e-03, -1.16993180e-02,\n",
       "         -5.81646059e-03,  6.37512375e-03,  1.43576111e-03,\n",
       "         -9.19539947e-03,  7.54742557e-03,  1.19081307e-02,\n",
       "          1.92518402e-02, -1.45300559e-03,  5.42369206e-03,\n",
       "         -1.20910760e-02,  1.60249583e-02]], dtype=float32)>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SentimentalClassifier(encoder=encoder, dense_units=64)\n",
    "model.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
    "\n",
    "sample_text = ('The movie was cool. The animation and the graphics '\n",
    "               'were out of this world. I would recommend this movie.')\n",
    "model(np.array([sample_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, epochs=10,\n",
    "            validation_data=test_dataset,\n",
    "            validation_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
